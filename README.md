# ReLU-solves-XOR
The XOR function is a classic example of a problem that cannot be solved with a linear classifier.The Rectified Linear Unit (ReLU) activation function can be used in a neural network to introduce non-linearity and allow for the separation of classes in the XOR problem
