# ReLU-solves-XOR
The XOR function is a classic example of a problem that cannot be solved with a linear classifier.The Rectified Linear Unit (ReLU) activation function can be used in a neural network to introduce non-linearity and allow for the separation of classes in the XOR problem
ReLU activation function is defined as follows:
To demonstrate how a neural network with ReLU activation can solve the XOR problem, we can use a simple two-layer network. Let's define the input layer with two neurons corresponding to the coordinates (x1, x2), a hidden layer with two neurons, and an output layer with a single neuron representing the class
Input Layer: Two neurons corresponding to the coordinates (x1, x2).
Hidden Layer: Two neurons with ReLU activation.
Output Layer: One neuron with a sigmoid activation (for binary classification).

